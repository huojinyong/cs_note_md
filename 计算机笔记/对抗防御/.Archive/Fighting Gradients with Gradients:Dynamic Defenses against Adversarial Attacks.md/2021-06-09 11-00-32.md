---
title: Fighting Gradients with Gradients:Dynamic Defenses against Adversarial Attacks
date: 2021-06-07 19:24
---
| A | B |
:--- | :--- 
迭代攻击 | 
自适应攻击 | 

# 当前防御的问题
 现在的防御是静态的，训练好后就不变了，在测试阶段不能自适应。  
> `测试阶段`就是攻击阶段，测试集中包含对抗样本
`随机性防御`能够在测试阶段改变网络或者输入，但其随机性与测试集（输入）本身无关，因此本质仍是静态防御，这与*确定性防御*一致。   
静态防御缺点：当`orange:数据`和**攻击**（方法）发生变化时，它还不变。针对一种攻击设计的防御方法对另一种攻击可能效果较差。  
`对抗训练`有很多弊端：需要大量参数和优化，会导致分类器准确度下降等。因此本文主要面向测试阶段的自适应，而非关注训练阶段。  

# 自适应的防御与攻击  
**自适应的防御（dent）**：每次对输入样本进行分类后，都要：1、对新**输入**进行新的平滑处理$\Sigma$；2、更新**模型**（分类器）参数$\theta + \Delta$    
![](./_image/2021-06-08/2021-06-08-20-04-00@2x.png)  

> $\Sigma$和$\Delta$取决于测试数据，所以 dent 是动态的。  
特点：
- 测试时期防御
- 自适应模型与输入

**自适应的攻击**：一个自然图像加上初始扰动作为初始的对抗样本，用它攻击分类器，得到损失值$L(f(x + \delta),y)$，用其更新扰动，把新的扰动加入自然图像上得到更新的对抗样本，如此**迭代循环进行攻击**，最终可得到完美的对抗样本。这个过程就是**自适应的**。  
![](./_image/2021-06-08/2021-06-08-19-49-26@2x.png)    
$H(\hat{y}) = - \sum_{c \in 1,...,C} p(\hat{y_c}logp(\hat{y_c}))$