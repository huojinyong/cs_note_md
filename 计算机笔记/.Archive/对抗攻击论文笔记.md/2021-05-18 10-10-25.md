---
title: 对抗攻击论文笔记
date: 2021-05-04 20:47
---
目录
[TOC]


# 1_FGSM
FGSM是对抗学习的经典

#### **扰动的计算方法**：
（分类器）模型的损失函数对输入的梯度经过sign函数后乘以$\epsilon$，即$\eta$:
$\eta = \epsilon sign(\nabla_x J(\pmb{\theta},\pmb{x} ,y))$
> 上面说的梯度不就是权重吗！！因为线性模型为w\*x，那它对x求导不就等于w吗！！原来如此哦！
这就是所谓的“快速梯度符号方法”，人如其名。

`yellow:ps:梯度可以理解为正的：加上（+）梯度会使损失函数变大，减去（-）梯度会使损失函数变小。`
<br>
#### **训练中使用基于FGSM的对抗目标函数（应该是损失函数）可以实现正则化:**
$\tilde{J}( \pmb{\theta},\pmb{x},y) = \alpha J(\pmb{ \theta}, \pmb{x},y)+(1-\alpha)J(\pmb{ \theta}, \pmb{x} +\epsilon sign(\nabla_x J(\pmb{ \theta},\pmb{x} ,y)).$  

**把对抗性样本和干净的样本混合起来训练，可以使神经网络正则化。**


#### 容量低的模型无法做出许多不同的自信的预测（并不是大多数模型都这样！）
比如**浅层**RBF网络只能自信的预测$\mu$附近存在的正（分类）样本：
$p(y=1|x)=exp(({x} - \mu )^T\beta({x}-\mu))$
RBF网络天然能抵抗对抗示例，因为他被欺骗时给出的置信度很低。
<br>
- 易于优化的模型容易被对抗示例攻击成功
- 线性行为是导致对抗样例跨模型泛化的主要因素
- **快速梯度符号法的核心思想是：沿着梯度的反方向添加扰动从而拉大对抗样例与原始样本的距离**
> 个人理解：沿着梯度的**反方向**添加扰动可以使损失函数值变大，从而使分类器错误分类。
- Goodfellow认为对抗样例之所以有泛化性的原因是因为添加的扰动与模型的权重向量高度一致，而且不同的模型在被训练执行相同的任务时，从训练数据中学到的东西相似。
> “*从训练数据中学到的东西相似*”就是说不同模型的权重很相似，因为权重蕴含着知识。
- 缺点：这是一种不定向的攻击，只能让模型出错而无法做到定向攻击
- 缺点：这种攻击的鲁棒性不强，添加的扰动容易在图片的预处理阶段被过滤掉
---

# 2_Adversarial machine learning at scale

|作者 | 位置 | 年份|
 :---: |  :---:  |  :---: 
Alexey Kurakin | ICLR | 2017
Ian J. **Goodfellow** | 
Samy Bengio| 

动机：对 FGSM 攻击方法的改进，提出了 BIM 攻击算法。

**对抗训练**：用对抗样本去训练模型，使得模型对对抗攻击具有鲁棒性，降低模型在自然数据样本上的错误率。
目前，对抗训练只要应用于小问题，**本文要将对抗训练扩展到大型模型和数据集上**。

本文几点贡献：
- 将对抗训练扩展到大型模型和数据集上
- 发现对抗训练赋予**单步攻击方法**鲁棒性
- 发现**多步攻击方法**的可移植性比单步攻击方法要**差**，因此单步攻击方法最适合发动**黑盒攻击**
- 经过对抗训练的模型在**对抗样本**上的表现要好于**纯样本**（`red:不明白`）

模型对`对抗样本`误分类的概率远远大于对`被（巨大）噪声扰动的样本`误分类的概率，可见`对抗样本`含有*某些本质的原因*！

有工作侧重研究**对抗样本在不同类型类型的模型上转移的怎么样**的问题，而本文侧重研究**不同类型的对抗样本生成过程在相对相似的模型之间转移的怎么样**的问题。

# 3_Towards Evaluating the Robustnessof Neural Networks   
本文贡献：提出了 著名的C&W 算法来攻击当时无人能敌的蒸馏防御。  
攻击方式：白盒攻击（perfect-knowledge 攻击）  

## 基本概念  
**评估 NN 健壮性：  **
![](./_image/2021-05-17/2021-05-17-16-05-06@2x.jpg)  
如果攻击不强，则 *构造证明上限的攻击* 无效。本文就是要生成攻击来构造 NN 的鲁棒性上限。  
- CIFAR-10 数据集也有 10 个类别。  
- ImageNet 数据集有 1000 个类别，这个数据集很大很难。  
`green:注意`**目标函数的选择**对找到对抗样本至关重要。  
**音频也可以成为对抗样本**。如人类给出语音命令让 iPhone 播放视频，但手机却打开网页下载文件。  

## 符号定义   
**模型**：$F(x) = y$
**输入**：$x \in R^n$  
**输出**：$y \in R^m$  
**结果标签**：$C(x) = arg\ max_iF(x)_i$  
真实标签：$C^*(x)$
**NN 结构**：
**... -> 最后一层隐藏层 -> $logits$ ->（输入到 $softmax$ 函数） -> 输出 $F(x) $  **
$Z(x) = x$是除了$softmax$层的所有层的输出，所以z 是$logits$  
$F(x) = softmax(Z(x)) = y$  
![](./_image/2021-05-17/2021-05-17-21-22-19@2x.png)
>  应该很容易理解  

选择 target class(目标类，即人为使得 x'被误分类为哪一类)的三种方法：
1. average case:随机选一个非正确类作为 target class
2. best case:对所有非正确类进行攻击，选择最容易攻击的那个类作为 target class
3. worst case:对所有非正确类进行攻击，选择最难攻击的那个类作为 target class  
`orange:疑问``Notice  thatif a classifier is only accurate80%of the time, then the bestcase attack will require a change of0in20%of cases.(请注意，如果分类器仅在80％的时间内是准确的，则最佳情况下的攻击将需要在20％的情况下将其更改为0)`**是什么意思？**  
**$L$范数（距离）用来衡量$x$和$x'$的差异**
1、 $L_0$距离是指$x_i\neq x_{i}'$的$i$的数量，即真实图片和对抗样本图片中不相等的像素的个数