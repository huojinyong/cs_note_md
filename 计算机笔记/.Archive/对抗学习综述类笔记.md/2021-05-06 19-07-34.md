---
title: 对抗学习综述类笔记
date: 2021-05-06 15:15
---

# Adversarial Examples Are Not Easily Detected:Bypassing Ten Detection Methods
- **摘要** 已知神经网络容易受到对抗性样本的攻击：输入接近自然输入但分类错误。为了更好地理解对抗性例子的空间，我们提供了十个旨在检测和比较其有效性的最新建议。我们证明，通过构造新的损失函数可以克服一切。我们得出的结论是，对抗性示例比以前意识到的要难得多，并且事实上，对抗性示例所固有的属性实际上并没有。最后，我们提出了一些简单的准则来评估未来提出的防御措施
- - - - - -- 
单词解释
- distortion（失真）：图像扭曲程度。失真越小图像越清晰、真实。

- - - - - -- 

研究对象：用于图像分类的神经网络

本文反对"对抗样本与自然图像具有内在差异的假设"

1. 不采取措施的通用攻击
2. 白盒攻击，有针对性
3. 可转移

## 符号定义
$F(·)$ : 神经网络分类器
${F(x)}_i$  : 预测分类为第 i 类的概率
$F^i(x) = RELU(A^i·F^{i-1}(x) + b)$ : 第 i 层输出
$Z(x) = F^n(x)$ : 最后一层（第 n 层）输出
$F(x) = softmax(Z(x))$ : 神经网络的最终输出（因为 Z(x)还要经过神经网络的唯一终极 softmax 层处理为概率形式）
$C(x) = arg &thinsp; max_i(F(x)_i)$ ： $F(·)$在对x 上给出的分类。  

神经网络很**健壮**意味着很难在其上找到对抗性例子  
对对抗示例进行正确分类很难，因此现在的对抗防御（defense）转而去做检测对抗示例并拒绝他们。`我感觉"拒绝"就是 拒绝对他们进行分类，傲娇~`

三种模型
- 零知识对手：什么也不知道
- 全知全能型对手：知道神经网络被检测器 D 保护，且知道 D 的模型参数
- 一知半解型对手：知道神经网络被检测器 D 保护，但不能访问经过训练的检测器 D   

### 生成对抗样本
使用 $L_2$攻击算法去生成**目标对抗样本**（定向攻击）。  
l($x'$) = max(max{$Z($x'$)_i:i{$\neq$}$})

- Q：为什么k 可以调节模型分类器对对抗样本的置信度？
- A：







