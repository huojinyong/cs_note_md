---
title: 对抗学习综述类笔记
date: 2021-05-06 15:15
---
[TOC]
- [Adversarial Examples Are Not Easily Detected:Bypassing Ten Detection Methods](#adversarial-examples-are-not-easily-detected-bypassing-ten-detection-methods)
    - [架构](#架构)
  - [符号定义](#符号定义)
    - [生成对抗样本](#生成对抗样本)
    - [实施攻击](#实施攻击)
      - [自适应的白盒攻击](#自适应的白盒攻击)
      - [limited-knowledge 的黑盒攻击](#limited-knowledge的黑盒攻击)

brown:主线
orange:不懂/存疑

# Adversarial Examples Are Not Easily Detected:Bypassing Ten Detection Methods
- **摘要** 已知神经网络容易受到对抗性样本的攻击：输入接近自然输入但分类错误。为了更好地理解对抗性例子的空间，我们提供了十个旨在检测和比较其有效性的最新建议。我们证明，通过构造新的损失函数可以克服一切（攻破一切防御）。我们得出的结论是，对抗性示例比以前意识到的要难得多，并且事实上，对抗性示例所固有的属性实际上并没有。最后，我们提出了一些简单的准则来评估未来提出的防御措施
- - - - - -- 
单词解释
- distortion（失真）：图像扭曲程度。失真越小图像越清晰、真实。

- - - - - -- 

## 架构
对手生成 *对抗样本* ，欺骗 *分类器* （神经网络），而分类器装配了 *检测器* ，用来检测样本是否是生成的对抗样本（而不是自然样本）。
- - - - - -- 
**研究对象**：用于图像分类的神经网络

本文反对"对抗样本与自然图像具有内在差异的假设"

1. 不采取措施的通用攻击
2. 白盒攻击，有针对性
3. 可转移

## 符号定义
$F(·)$ : 神经网络分类器
${F(x)}_i$  : 预测分类为第 i 类的概率
$F^i(x) = RELU(A^i·F^{i-1}(x) + b)$ : 第 i 层输出
$Z(x) = F^n(x)$ : 最后一层（第 n 层）输出
$F(x) = softmax(Z(x))$ : 神经网络的最终输出（因为 Z(x)还要经过神经网络的唯一终极 softmax 层处理为概率形式）
$C(x) = arg &thinsp; max_i(F(x)_i)$ ： $F(·)$在对x 上给出的分类。  

神经网络很**健壮**意味着很难在其上找到对抗性例子  
对对抗示例进行正确分类很难，因此现在的对抗防御（defense）转而去做检测对抗示例并拒绝他们。`我感觉"拒绝"就是 拒绝对他们进行分类，傲娇~`

三种模型
- 零知识对手：什么也不知道
- 全知全能型对手：知道神经网络被检测器 D 保护，且知道 D 的模型参数
- 一知半解型对手：知道神经网络被检测器 D 保护，但不能访问经过训练的检测器 D   

## 生成对抗样本
Carlini 和 Wagner（**C&W**）使用 $L_2$攻击算法去生成**目标对抗样本**（定向攻击）:
给定神经网络的$logits &thinsp; Z$，攻击使用梯度下降进行：
$minimize ||x' -  x||_2^2 + c·l(x')$
损失函数$l$定义为：
$l(x') = max(max[Z(x')_i:i \neq t] - Z(x')_t, -k)$

$k = 0$ 时，对抗样本被称为 *低置信度样本* ，它直接被分为目标类。$k \neq 0$时，对抗样本被称为 *高置信度样本* ，分类器在将它分类时更自信。
**Q**：为什么k 可以调节模型分类器对对抗样本的置信度？
**A**：~~当 k 增加时，如-k = -1000 时，$l(x')$若只有$-k$则暗示损失值非常低（是负数，-1000），如果真实的$l(x')$仍然是负数，说明非目标类的最大概率值与~~
`orange:不懂`

## 实施攻击
用三种攻击方式（前文有述）攻击防御方。  
`blue:猜测`adaptive attack（自适应攻击）：是指 x' = x + $\alpha \nabla$中的$\alpha$不是固定的。  

### 自适应的白盒攻击
`green:重点`构造损失函数，其可以生成对抗样本。  

### limited-knowledge 的黑盒攻击  
对手知道防御的类型但不知道检测器的参数。
攻击方法：训练和待攻击模型相似的代理模型，通过对代理模型发动白盒攻击从而生成对抗样本，再将其转移给原始模型即可。

## 评估防御措施
`orange:存疑`second classifier 好像是指分类器和检测器二合一的分类器。
$F(·)$是分类网络
$D(·)$是检测网络
$sigmoid(D(x)) \in  [0,1] $ 表示实例（instance）是对抗样本的概率。

### 第一种检测方法
构建一个 second classifier 来检测对抗样本  

#### 两种防御方法
##### 1.对抗再训练     
**对抗再训练**  (Adversarial Retraining)       
 ** Grosse ** 等提出了新思路：

| 传统方法 | 新方法 |
:---: | :---:
把对抗样本及其标签加入训练集训练，使分类器将其正确分类 | 引入第 N+1 类，该类表示对抗样本类  

流程如下：  
![](./_image/2021-05-09/2021-05-09-10-50-10@2x.png)  
`pink：总结`先训练好模型，在该模型上生成对抗样本，将对抗样本集和原样本集加入数据集里，再次训练模型即可。**对抗再训练**可以识别出输入样本是否为对抗样本（ *即第 N+1 类样本* ）
$F_{secured}$是一个(有)防御(能力的)模型，既能够对输入样本进行正确分类，也能够识别出输入样本是否为对抗样本（这就是防御的含义啊！）

##### 2.构建二元分类器     
**Gong ** 等提出了类似的方法： **构建二元分类器 D** （`我感觉从功能上讲它就是鉴别器`），自然样本标记为 1，对抗样本标记为 0。训练集如下：
![](./_image/2021-05-09/2021-05-09-10-57-35@2x.png)  
结果发现，在 FGSM 和 JSMA 攻击（比 **C&W**弱）时，**对抗再训练**可以检测出对抗样本！厉害！

`brown:主线`在 zero-knowledge 攻击评估（识别）中，我们用 **MNIST**数据集训练，对抗样本我们使用 C&W 的攻击方式生成，识别（即防御）方面，我们使用了上述两种防御技术。  
但是，上述两种防御技术在 CIFAR 数据集上失败。  

`orange ：问题`"进一步调查发现，即使我们训练使用无目标攻击生成的对抗示例，这两种方案也都可以检测到目标对抗示例"  什么意思？？？  

`brown:主线`perfect-knowledge 攻击评估。假设对手知道上述两种防御方法及其参数，结果表明防御无法抵挡 perfect-knowledge 攻击。  

之前我们做了在$F_{base}$（基础模型）上生成对抗样本的事情。现在我们在$F_{secured}$(防御模型)上再生成对抗样本，看看会发生什么。结果表明，**在$F_{secured}$上还是能够生成对抗样本，生成成功率并不比在$F_{base}$上的低**。唯一的区别是在前者上生成的样本的失真程度（肉眼看起来的扭曲程度）要比后者高 10%。  

##### **攻破 Gong 等的"构建二元分类器"防御方法**
定义一个 $G$函数整合前面的分类器$F$和鉴别器$D$：  
![](./_image/2021-05-09/2021-05-09-15-51-29@2x.png)  
其中$Z_F$是分类器$F$的最后一层输出，即还没经过 softmax 层（上面说过函数$Z(x)$的定义），$Z_D$是鉴别器$D$的最后一层输出。  
- 当对抗样本被 $D$识别为恶意示例时（$Z_D(x) > 0$），有：![](./_image/2021-05-09/2021-05-09-15-56-51@2x.png)
- 当对抗样本被 $D$识别为自然示例时（$Z_D(x) < 0$），有：![](./_image/2021-05-09/2021-05-09-15-57-41@2x.png)   
`orange:不懂`这样做的目的是让 G 有线性性质，使得某情况下的计算成为可能  
> 这个$G(x)_i$和$Z_F(x)_i$很类似，基本上是给出样本$x$经过 $NN$ 后的输出（尚未经过 $softmax $层）   

 ** 攻破 Gong 等防御方法的攻击方法：  **    
用$G(x)$替代$Z(x)$输入到 **C&W**的对抗样本生成方法中，就能产生 100%成功欺骗 Gong 等的防御方法的对抗样本了！  
这些对抗样本的平均$L_2$失真比未受保护的方法高 10%（挺好的了）。
上面👆这句话什么意思？
1. 虽然我们能成功攻破 Gong 等的防御模型，但是相对于攻击未受保护的模型，在前者模型上生成的对抗样例人眼看起来会稍微有些扭曲。但扭曲程度也就提升了 10%，挺好的了。
2. 讲一下这个流程到底是怎么回事，懂了可以跳过：有一个分类器模型， **C&W**想攻击他，所以在该模型上生成对抗样本，这样就可以（定向）欺骗这个模型了。注意，在 **C&W**的方法中，是需要知道被攻击模型的内部信息的，比如用到的Z(x)就是该模型最后一层的输出（第 n 层F^n(x)）。后来 Gong 等提出了防御方法，效果挺好。为了攻破 Gong，就有了上述的 *攻破 Gong 等防御方法的攻击方法* 。  
`blue:总结`这说明上述两种防御方法（Grosse、Gong）至少在 MNIST 上都不行。在 CIFAR 上结论相似，甚至失真更少了（说明攻击方实在是太狡猾太强大了！）   
`brown:主线`limited-knowledge 攻击评估。












