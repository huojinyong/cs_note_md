---
title: 对抗学习综述类笔记
date: 2021-05-06 15:15
---
[TOC]
# Adversarial Examples Are Not Easily Detected:Bypassing Ten Detection Methods
- **摘要** 已知神经网络容易受到对抗性样本的攻击：输入接近自然输入但分类错误。为了更好地理解对抗性例子的空间，我们提供了十个旨在检测和比较其有效性的最新建议。我们证明，通过构造新的损失函数可以克服一切。我们得出的结论是，对抗性示例比以前意识到的要难得多，并且事实上，对抗性示例所固有的属性实际上并没有。最后，我们提出了一些简单的准则来评估未来提出的防御措施
- - - - - -- 
单词解释
- distortion（失真）：图像扭曲程度。失真越小图像越清晰、真实。

- - - - - -- 

### 架构
对手生成 *对抗样本* ，欺骗 *分类器* （神经网络），而分类器装配了 *检测器* ，用来检测样本是否是生成的对抗样本（而不是自然样本）。
- - - - - -- 
**研究对象**：用于图像分类的神经网络

本文反对"对抗样本与自然图像具有内在差异的假设"

1. 不采取措施的通用攻击
2. 白盒攻击，有针对性
3. 可转移

## 符号定义
$F(·)$ : 神经网络分类器
${F(x)}_i$  : 预测分类为第 i 类的概率
$F^i(x) = RELU(A^i·F^{i-1}(x) + b)$ : 第 i 层输出
$Z(x) = F^n(x)$ : 最后一层（第 n 层）输出
$F(x) = softmax(Z(x))$ : 神经网络的最终输出（因为 Z(x)还要经过神经网络的唯一终极 softmax 层处理为概率形式）
$C(x) = arg &thinsp; max_i(F(x)_i)$ ： $F(·)$在对x 上给出的分类。  

神经网络很**健壮**意味着很难在其上找到对抗性例子  
对对抗示例进行正确分类很难，因此现在的对抗防御（defense）转而去做检测对抗示例并拒绝他们。`我感觉"拒绝"就是 拒绝对他们进行分类，傲娇~`

三种模型
- 零知识对手：什么也不知道
- 全知全能型对手：知道神经网络被检测器 D 保护，且知道 D 的模型参数
- 一知半解型对手：知道神经网络被检测器 D 保护，但不能访问经过训练的检测器 D   

### 生成对抗样本
使用 $L_2$攻击算法去生成**目标对抗样本**（定向攻击）:
给定神经网络的$logits &thinsp; Z$，攻击使用梯度下降进行：
$minimize ||x' -  x||_2^2 + c·l(x')$
损失函数$l$定义为：
$l(x') = max(max[Z(x')_i:i \neq t] - Z(x')_t, -k)$

$k = 0$ 时，对抗样本被称为 *低置信度样本* ，它直接被分为目标类。$k \neq 0$时，对抗样本被称为 *高置信度样本* ，分类器在将它分类时更自信。
**Q**：为什么k 可以调节模型分类器对对抗样本的置信度？
**A**：~~当 k 增加时，如-k = -1000 时，$l(x')$若只有$-k$则暗示损失值非常低（是负数，-1000），如果真实的$l(x')$仍然是负数，说明非目标类的最大概率值与~~
`red:不懂`

### 实施攻击
用三种攻击方式（前文有述）攻击防御方。  
`blue:猜测`adaptive attack（自适应攻击）：是指 x' = x + $\alpha \nabla$中的$\alpha$不是固定的。  

#### 自适应的**白盒攻击**
`green:重点`构造损失函数，其可以生成对抗样本。  

#### 









