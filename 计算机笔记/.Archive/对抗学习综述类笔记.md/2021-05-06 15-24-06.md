---
title: 对抗学习综述类笔记
date: 2021-05-06 15:15
---

# Adversarial Examples Are Not Easily Detected:Bypassing Ten Detection Methods
- **摘要** 已知神经网络容易受到对抗性样本的攻击：输入接近自然输入但分类错误。为了更好地理解对抗性例子的空间，我们提供了十个旨在检测和比较其有效性的最新建议。我们证明，通过构造新的损失函数可以克服一切。我们得出的结论是，对抗性示例比以前意识到的要难得多，并且事实上，对抗性示例所固有的属性实际上并没有。最后，我们提出了一些简单的准则来评估未来提出的防御措施

研究对象：用于图像分类的神经网络

本文否认""
